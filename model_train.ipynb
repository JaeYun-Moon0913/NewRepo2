{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cnn_model.Vgg import VGG \n",
    "from cnn_model.alexnet import AlexNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.is_available() -> bool>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
       "   (1): ReLU()\n",
       "   (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "   (1): ReLU()\n",
       "   (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ),\n",
       " Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Sequential(\n",
       "   (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (1): ReLU()\n",
       "   (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ),\n",
       " Linear(in_features=9216, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=4096, bias=True),\n",
       " Linear(in_features=4096, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "alex_model = AlexNet().to(DEVICE) # input img size = (227,227)\n",
    "list(alex_model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (1): ReLU()\n",
       "   (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (3): ReLU()\n",
       "   (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (1): ReLU()\n",
       "   (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (3): ReLU()\n",
       "   (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (1): ReLU()\n",
       "   (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (3): ReLU()\n",
       "   (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (5): ReLU()\n",
       "   (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (1): ReLU()\n",
       "   (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (3): ReLU()\n",
       "   (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (5): ReLU()\n",
       "   (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (1): ReLU()\n",
       "   (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (3): ReLU()\n",
       "   (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (5): ReLU()\n",
       "   (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Dropout(p=0.5, inplace=False)\n",
       "   (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "   (4): ReLU()\n",
       "   (5): Dropout(p=0.5, inplace=False)\n",
       "   (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "   (7): ReLU()\n",
       "   (8): Dropout(p=0.5, inplace=False)\n",
       "   (9): Linear(in_features=1000, out_features=10, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg16_model = VGG(3,10,'vgg16',input_size=32,batchnorm=False).to(DEVICE)\n",
    "list(vgg16_model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc_layer'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model._modules.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import os \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "import torchvision.transforms as transforms \n",
    "from torchvision import datasets\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(32),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.491,0.482, 0.447), (0.247, 0.243, 0.261)),\n",
    "# ])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train = True, download=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True,num_workers = 2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train = False, download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False,num_workers = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.49139968 0.48215841 0.44653091]\n",
    "[0.24703223 0.24348513 0.26158784]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Custom data torchvision(tensor) dataset make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./cifar-10-batches-py/train\\\\airplane',\n",
       " './cifar-10-batches-py/train\\\\automobile',\n",
       " './cifar-10-batches-py/train\\\\bird',\n",
       " './cifar-10-batches-py/train\\\\cat',\n",
       " './cifar-10-batches-py/train\\\\deer',\n",
       " './cifar-10-batches-py/train\\\\dog',\n",
       " './cifar-10-batches-py/train\\\\frog',\n",
       " './cifar-10-batches-py/train\\\\horse',\n",
       " './cifar-10-batches-py/train\\\\ship',\n",
       " './cifar-10-batches-py/train\\\\truck']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob \n",
    "dataset_pth = glob.glob('./cifar-10-batches-py/train/*')\n",
    "\n",
    "dataset_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'automobile': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'deer': 4,\n",
       " 'dog': 5,\n",
       " 'frog': 6,\n",
       " 'horse': 7,\n",
       " 'ship': 8,\n",
       " 'truck': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.491,0.482, 0.447), (0.247, 0.243, 0.261))])\n",
    "\n",
    "image_fd = datasets.ImageFolder(root = './cifar-10-batches-py/train',\n",
    "                                transform=transform)\n",
    "\n",
    "test_fd = datasets.ImageFolder(root = './cifar-10-batches-py/test',\n",
    "                                transform=transform)\n",
    "\n",
    "class_list= image_fd.class_to_idx\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(image_fd,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_fd,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 32, 32]), torch.Size([32]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Train_datasets----- \n",
      " Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=224, interpolation=bilinear)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.491, 0.482, 0.447), std=(0.247, 0.243, 0.261))\n",
      "           )\n",
      "-----Test_datasets----- \n",
      " Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=224, interpolation=bilinear)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.491, 0.482, 0.447), std=(0.247, 0.243, 0.261))\n",
      "           )\n",
      "------------------------------\n",
      "Shape of X [N, C, H, W]: torch.Size([64, 3, 224, 224])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"-----Train_datasets----- \\n\",trainset)\n",
    "print(\"-----Test_datasets----- \\n\",testset)\n",
    "\n",
    "for X, y in trainloader:\n",
    "    print('-'*30)\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(vgg16_model.parameters(),lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 2.303\n",
      "[1,   100] loss: 2.304\n",
      "[1,   150] loss: 2.304\n",
      "[1,   200] loss: 2.304\n",
      "[1,   250] loss: 2.304\n",
      "[1,   300] loss: 2.304\n",
      "[1,   350] loss: 2.303\n",
      "[1,   400] loss: 2.304\n",
      "[1,   450] loss: 2.241\n",
      "[1,   500] loss: 2.121\n",
      "[1,   550] loss: 2.055\n",
      "[1,   600] loss: 1.991\n",
      "[1,   650] loss: 1.959\n",
      "[1,   700] loss: 1.908\n",
      "[1,   750] loss: 1.920\n",
      "[1,   800] loss: 1.900\n",
      "[1,   850] loss: 1.906\n",
      "[1,   900] loss: 1.915\n",
      "[1,   950] loss: 1.886\n",
      "[1,  1000] loss: 1.866\n",
      "[1,  1050] loss: 1.840\n",
      "[1,  1100] loss: 1.862\n",
      "[1,  1150] loss: 1.859\n",
      "[1,  1200] loss: 1.812\n",
      "[1,  1250] loss: 1.844\n",
      "[1,  1300] loss: 1.763\n",
      "[1,  1350] loss: 1.769\n",
      "[1,  1400] loss: 1.728\n",
      "[1,  1450] loss: 1.667\n",
      "[1,  1500] loss: 1.691\n",
      "[1,  1550] loss: 1.715\n",
      "[2,    50] loss: 1.684\n",
      "[2,   100] loss: 1.678\n",
      "[2,   150] loss: 1.627\n",
      "[2,   200] loss: 1.712\n",
      "[2,   250] loss: 1.664\n",
      "[2,   300] loss: 1.636\n",
      "[2,   350] loss: 1.595\n",
      "[2,   400] loss: 1.664\n",
      "[2,   450] loss: 1.605\n",
      "[2,   500] loss: 1.588\n",
      "[2,   550] loss: 1.610\n",
      "[2,   600] loss: 1.595\n",
      "[2,   650] loss: 1.610\n",
      "[2,   700] loss: 1.525\n",
      "[2,   750] loss: 1.539\n",
      "[2,   800] loss: 1.543\n",
      "[2,   850] loss: 1.567\n",
      "[2,   900] loss: 1.526\n",
      "[2,   950] loss: 1.484\n",
      "[2,  1000] loss: 1.487\n",
      "[2,  1050] loss: 1.439\n",
      "[2,  1100] loss: 1.494\n",
      "[2,  1150] loss: 1.452\n",
      "[2,  1200] loss: 1.423\n",
      "[2,  1250] loss: 1.411\n",
      "[2,  1300] loss: 1.441\n",
      "[2,  1350] loss: 1.467\n",
      "[2,  1400] loss: 1.430\n",
      "[2,  1450] loss: 1.402\n",
      "[2,  1500] loss: 1.385\n",
      "[2,  1550] loss: 1.355\n",
      "[3,    50] loss: 1.403\n",
      "[3,   100] loss: 1.386\n",
      "[3,   150] loss: 1.363\n",
      "[3,   200] loss: 1.316\n",
      "[3,   250] loss: 1.344\n",
      "[3,   300] loss: 1.266\n",
      "[3,   350] loss: 1.340\n",
      "[3,   400] loss: 1.300\n",
      "[3,   450] loss: 1.315\n",
      "[3,   500] loss: 1.302\n",
      "[3,   550] loss: 1.286\n",
      "[3,   600] loss: 1.269\n",
      "[3,   650] loss: 1.325\n",
      "[3,   700] loss: 1.239\n",
      "[3,   750] loss: 1.234\n",
      "[3,   800] loss: 1.276\n",
      "[3,   850] loss: 1.192\n",
      "[3,   900] loss: 1.244\n",
      "[3,   950] loss: 1.261\n",
      "[3,  1000] loss: 1.221\n",
      "[3,  1050] loss: 1.241\n",
      "[3,  1100] loss: 1.253\n",
      "[3,  1150] loss: 1.257\n",
      "[3,  1200] loss: 1.209\n",
      "[3,  1250] loss: 1.257\n",
      "[3,  1300] loss: 1.177\n",
      "[3,  1350] loss: 1.200\n",
      "[3,  1400] loss: 1.189\n",
      "[3,  1450] loss: 1.180\n",
      "[3,  1500] loss: 1.175\n",
      "[3,  1550] loss: 1.230\n",
      "[4,    50] loss: 1.132\n",
      "[4,   100] loss: 1.140\n",
      "[4,   150] loss: 1.115\n",
      "[4,   200] loss: 1.112\n",
      "[4,   250] loss: 1.124\n",
      "[4,   300] loss: 1.107\n",
      "[4,   350] loss: 1.056\n",
      "[4,   400] loss: 1.113\n",
      "[4,   450] loss: 1.181\n",
      "[4,   500] loss: 1.068\n",
      "[4,   550] loss: 1.131\n",
      "[4,   600] loss: 1.185\n",
      "[4,   650] loss: 1.124\n",
      "[4,   700] loss: 1.084\n",
      "[4,   750] loss: 1.057\n",
      "[4,   800] loss: 1.096\n",
      "[4,   850] loss: 1.100\n",
      "[4,   900] loss: 1.077\n",
      "[4,   950] loss: 1.057\n",
      "[4,  1000] loss: 1.080\n",
      "[4,  1050] loss: 1.028\n",
      "[4,  1100] loss: 1.111\n",
      "[4,  1150] loss: 1.120\n",
      "[4,  1200] loss: 1.077\n",
      "[4,  1250] loss: 1.117\n",
      "[4,  1300] loss: 1.085\n",
      "[4,  1350] loss: 1.097\n",
      "[4,  1400] loss: 1.082\n",
      "[4,  1450] loss: 1.033\n",
      "[4,  1500] loss: 1.023\n",
      "[4,  1550] loss: 1.010\n",
      "[5,    50] loss: 1.048\n",
      "[5,   100] loss: 0.993\n",
      "[5,   150] loss: 1.023\n",
      "[5,   200] loss: 1.016\n",
      "[5,   250] loss: 0.967\n",
      "[5,   300] loss: 1.044\n",
      "[5,   350] loss: 0.972\n",
      "[5,   400] loss: 1.010\n",
      "[5,   450] loss: 0.947\n",
      "[5,   500] loss: 0.946\n",
      "[5,   550] loss: 0.971\n",
      "[5,   600] loss: 0.997\n",
      "[5,   650] loss: 0.978\n",
      "[5,   700] loss: 0.969\n",
      "[5,   750] loss: 1.029\n",
      "[5,   800] loss: 1.004\n",
      "[5,   850] loss: 1.028\n",
      "[5,   900] loss: 0.926\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mjy22\\Desktop\\grad_result\\model_train.ipynb 셀 19\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mjy22/Desktop/grad_result/model_train.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m runnig_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mjy22/Desktop/grad_result/model_train.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# if (epoch>0):\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mjy22/Desktop/grad_result/model_train.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#     model = VGG16(num_classes = NUM_CLASS).to(DEVICE)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mjy22/Desktop/grad_result/model_train.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#     model.load_state_dict(torch.load(save_path))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mjy22/Desktop/grad_result/model_train.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#     model.to(DEVICE)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mjy22/Desktop/grad_result/model_train.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i,data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader,\u001b[39m0\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mjy22/Desktop/grad_result/model_train.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39minput\u001b[39m,labels \u001b[39m=\u001b[39m data \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mjy22/Desktop/grad_result/model_train.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39minput\u001b[39m,labels \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mto(DEVICE), labels\u001b[39m.\u001b[39mto(DEVICE)\n",
      "File \u001b[1;32mc:\\Users\\mjy22\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\mjy22\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\mjy22\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\mjy22\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\mjy22\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:230\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[1;32m--> 230\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[0;32m    231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\mjy22\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:269\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32mc:\\Users\\mjy22\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:249\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    248\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(f)\n\u001b[1;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\mjy22\\anaconda3\\envs\\pytorch\\lib\\site-packages\\PIL\\Image.py:901\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[0;32m    857\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[0;32m    858\u001b[0m ):\n\u001b[0;32m    859\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    903\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    904\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    905\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mjy22\\anaconda3\\envs\\pytorch\\lib\\site-packages\\PIL\\ImageFile.py:257\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m         )\n\u001b[0;32m    256\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 257\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    259\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_arr = [] \n",
    "for epoch in range(EPOCH):\n",
    "    runnig_loss = 0.0 \n",
    "\n",
    "    # if (epoch>0):\n",
    "    #     model = VGG16(num_classes = NUM_CLASS).to(DEVICE)\n",
    "    #     model.load_state_dict(torch.load(save_path))\n",
    "    #     model.to(DEVICE)\n",
    "\n",
    "    for i,data in enumerate(train_loader,0):\n",
    "        input,labels = data \n",
    "        input,labels = input.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs= vgg16_model(input)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        if(loss.item()>1000):\n",
    "            print(loss.item())\n",
    "            for param in vgg16_model.parametes():\n",
    "                print(param.data)\n",
    "                break\n",
    "\n",
    "        runnig_loss +=loss.item()\n",
    "        if i % 50 == 49:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, runnig_loss / 50))\n",
    "            loss_arr.append(runnig_loss)\n",
    "            runnig_loss = 0.0 \n",
    "    \n",
    "    #save_path=SAVE_PATH + \"cifar_vgg16.pth\"\n",
    "    #torch.save(vgg16_model.state_dict(), save_path)\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAME = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "NUM_CLASS = len(LABEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of airplane : 77 %\n",
      "Accuracy of automobile : 88 %\n",
      "Accuracy of  bird : 51 %\n",
      "Accuracy of   cat : 58 %\n",
      "Accuracy of  deer : 72 %\n",
      "Accuracy of   dog : 74 %\n",
      "Accuracy of  frog : 76 %\n",
      "Accuracy of horse : 80 %\n",
      "Accuracy of  ship : 91 %\n",
      "Accuracy of truck : 83 %\n",
      "Accuracy average:  75.30876799590796\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(NUM_CLASS))\n",
    "class_total = list(0. for i in range(NUM_CLASS))\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs= vgg16_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(NUM_CLASS):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "accuracy_sum=0\n",
    "for i in range(10):\n",
    "    temp = 100 * class_correct[i] / class_total[i]\n",
    "    print('Accuracy of %5s : %2d %%' % (LABEL_NAME[i], temp))\n",
    "\n",
    "    accuracy_sum+=temp\n",
    "print('Accuracy average: ', accuracy_sum/NUM_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vgg16_model,'./vgg_pt//vgg16_cifar10_30epoch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Datasets/Train_data/car\\000090.jpg\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import glob\n",
    "import cv2 \n",
    "from PIL import Image \n",
    "\n",
    "img_pth = glob.glob('./Datasets/Train_data/car/*.jpg')\n",
    "pth ='./Datasets/Train_data/car/train'\n",
    "\n",
    "\n",
    "print(img_pth[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in img_pth: \n",
    "    img = cv2.imread(i)\n",
    "    name = i.split('/')[3].split('\\\\')[1]\n",
    "    re_img = cv2.resize(img,dsize=(32,32),interpolation=cv2.INTER_AREA)\n",
    "    ful_name = os.path.join(pth,name)\n",
    "    cv2.imwrite(ful_name,re_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee22937dcbee3f51f6aed73815f5ebb41f6c896aa5905ae71496d4374e7de50a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
